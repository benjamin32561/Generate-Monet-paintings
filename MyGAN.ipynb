{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MyGAN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNdJ1CKnT7ELdYh93194K2S"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"IvZ548iNFB6r"},"source":["# 1 Discriminator"]},{"cell_type":"code","metadata":{"id":"dRUDIk-Ktnkg"},"source":["from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n","from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D, Add\n","from tensorflow.keras.layers import MaxPooling2D, UpSampling2D, Conv2D, Conv2DTranspose\n","from tensorflow.keras.initializers import RandomNormal\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.optimizers import SGD, Adam\n","import tensorflow as tf\n","from random import randint\n"," \n","from google.colab.patches import cv2_imshow\n","import numpy as np\n","from google.colab import drive\n"," \n","from tensorflow.python.framework.ops import disable_eager_execution, enable_eager_execution\n","\n","import random\n","\n","from tensorflow.keras.applications.vgg19 import VGG19\n","import glob"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AYhpfDF45iT5"},"source":["import logging\n","tf.get_logger().setLevel(logging.ERROR)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hBE7SeUKbUXu"},"source":["MODELS_PATH = \"/content/gdrive/MyDrive/Projects/GDL Images/models/MyGAN/\"\n","D_PATH = MODELS_PATH+\"disc.h5\"\n","G_PATH = MODELS_PATH+\"generator.h5\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBIbgao-eJw6"},"source":["drive.mount('/content/gdrive/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"marVKyisVpwt"},"source":["!ln -s \"/content/gdrive/MyDrive/Projects/GDL Images/dataset/monet.npy\" \"/content/monet.npy\"\n","!ln -s \"/content/gdrive/MyDrive/Projects/GDL Images/dataset/not/\" \"/content/\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LDw6kqv22zuU"},"source":["def loadNpy(path):\n","  return np.array(np.load(path, mmap_mode='r')/255,dtype=np.float64)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MWF8t2afsdKw"},"source":["class DataRemember():\n","  def __init__(self, maxsize=1000):\n","    self.maxsize = maxsize\n","    self.array = []\n","  \n","  def AddToArray(self, add):\n","    free_size = self.maxsize - len(self.array)\n","    to_remove = len(add) - free_size\n","    if to_remove > 0:\n","      self.array = self.array[to_remove:]\n","    for i in add:\n","      self.array.append(i)\n","    random.shuffle(self.array)\n","  \n","  def ToNumpy(self):\n","    return np.array(self.array)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JNlcj4-T9hYT"},"source":["def buildGeneratorVGG19(input_shape, weight_init):\n","  vgg19 = VGG19(weights='imagenet',include_top=False)\n","  def downSample(input_layer,filters,amount,max_pool=True):\n","    y = input_layer\n","    for _ in range(amount):\n","      y = Conv2D(filters,kernel_size=3, strides=1, padding='same', activation=\"relu\")(y)\n","    if max_pool:\n","      y = MaxPooling2D(pool_size=2)(y)\n","    return y\n","  def upSample(input_layer,filters,amount,weight_init):\n","    y = UpSampling2D(size=(2,2))(input_layer)\n","    for _ in range(amount):\n","      y = Conv2D(filters,kernel_size=3, strides=1, padding='same', activation=\"relu\",kernel_initializer=weight_init)(y)\n","      y = BatchNormalization()(y)\n","      y = Dropout(0.08)(y)\n","    return y\n","  input_layer = Input(shape=input_shape)\n","  block1_out = downSample(input_layer,64,2)\n","  block2_out = downSample(block1_out,128,2)\n","  block3_out = downSample(block2_out,256,4)\n","  block4_out = downSample(block3_out,512,4)\n","  block5_out = downSample(block4_out,512,4,False)\n","  y = Concatenate()([block5_out,block4_out])\n","  y = upSample(y,512,4,weight_init)\n","  y = Concatenate()([block3_out,y])\n","  y = upSample(y,256,4,weight_init)\n","  y = Concatenate()([block2_out,y])\n","  y = upSample(y,128,2,weight_init)\n","  y = Concatenate()([block1_out,y])\n","  y = upSample(y,64,2,weight_init)\n","  output = Conv2D(3,kernel_size=3, strides=1, padding='same', activation=\"sigmoid\",kernel_initializer=weight_init)(y)\n","\n","  model = Model(input_layer,output)\n","\n","  vgg19_layers = vgg19.layers\n","  model_layers = model.layers\n","  i = 0\n","  while i<len(vgg19_layers)-1:\n","    model_layers[i].set_weights(vgg19_layers[i].get_weights())\n","    model_layers[i].trainable = False\n","    i+=1\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"smnB2GmrDdfx"},"source":["def buildDiscriminator(input_shape,weight_init):\n","  def conv4(layer_input,filters,weight_init,norm=True,stride=2):\n","    y = Conv2D(filters, kernel_size=4, strides=stride, padding='same', kernel_initializer = weight_init, activation=\"relu\")(layer_input)\n","    if norm:\n","      y = BatchNormalization()(y)\n","    return y\n","\n","  img = Input(shape=input_shape)\n","\n","  y = conv4(img, 8, weight_init, norm = False)\n","  y = conv4(y, 16, weight_init)\n","  y = MaxPooling2D()(y)\n","  y = conv4(y, 32, weight_init)\n","  y = MaxPooling2D()(y)\n","  output = Conv2D(1, kernel_size=2, strides=2, padding='same',kernel_initializer = weight_init, activation=\"sigmoid\")(y)\n","\n","  return Model(img, output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gppEPwbiCqS8"},"source":["class MyGAN():\n","  def __init__(self, input_dim, disc_path, gen_path, load_models=False):\n","    weight_init = RandomNormal(mean=0., stddev=0.02)\n","    self.input_shape = input_dim\n","    if not load_models:\n","      self.generator = buildGeneratorVGG19(self.input_shape, weight_init)\n","    else:\n","      self.generator = tf.keras.models.load_model(gen_path)\n","    self.generator.trainable = True\n","    if not load_models:\n","      self.discriminator = buildDiscriminator(self.input_shape, weight_init)\n","    else:\n","      self.discriminator = tf.keras.models.load_model(disc_path)\n","    \n","    self.d_path = disc_path\n","    self.g_path = gen_path\n","    self.queue = DataRemember()\n","    output_shape_disc = self.discriminator.output.shape\n","    self.patch = output_shape_disc[1:]\n","    self.CompileModels()\n","\n","  def CompileModels(self):\n","    self.discriminator.compile(\n","        loss=\"mse\",\n","        optimizer=Adam(),\n","        metrics=['accuracy']\n","    )\n","    img = Input(shape=self.input_shape)\n","    generated = self.generator(img)\n","    output = self.discriminator(generated)\n","\n","    self.combined = Model(img,output)\n","    self.combined.compile(\n","        loss=\"mse\",\n","        optimizer=SGD(),\n","        metrics=['accuracy']\n","    )\n","  \n","  def TrainDiscriminator(self):\n","    self.discriminator.trainable = True\n","    print(\"Training discriminator...\")\n","    print(\" Generating data...\")\n","    generated = self.generator.predict(self.data_B)\n","    self.queue.AddToArray(generated)\n","    generated = self.queue.ToNumpy()\n","    print(\" Finished generating data...\")\n","    zeros = np.zeros((generated.shape[0]+self.data_B.shape[0],)+self.patch)\n","    x = np.concatenate((self.data_A,generated,self.data_B))\n","    y = np.concatenate((self.disc_ones,zeros))\n","    self.discriminator.fit(x,y,shuffle=True,epochs=5,batch_size=32)\n","\n","  def TrainGenerator(self):\n","    self.discriminator.trainable = False\n","    print(\"Training generator...\")\n","    y = np.ones((self.data_B.shape[0],)+self.patch)\n","    x = self.data_B\n","    self.combined.fit(x,y,epochs=1,batch_size=4)\n","\n","  def ShowProgress(self):\n","    i = random.randint(0, self.data_B.shape[0])\n","    img = np.array([self.data_B[i]])\n","    generated = np.array(self.generator.predict(img)[0]*255,dtype=np.int64)\n","    img = img[0]*255\n","    cv2_imshow(img)\n","    cv2_imshow(generated)\n","\n","  def saveModels(self):\n","    self.discriminator.trainable = True\n","    self.discriminator.save(self.d_path)\n","    self.generator.save(self.g_path)\n","    print(\"saved\")\n","\n","  def train(self, path_A, path_B, times=5):\n","    files = glob.glob(path_B)\n","    random.shuffle(files)\n","    self.data_A = loadNpy(path_A)\n","    self.disc_ones = np.ones((self.data_A.shape[0],)+self.patch)\n","    for i in range(1,times+1):\n","      print(\"{0}/{1}\".format(i,times))\n","      for B_file in files:\n","        self.data_B = loadNpy(B_file)\n","        self.TrainDiscriminator()\n","        self.TrainGenerator()\n","        self.ShowProgress()\n","        self.saveModels()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OlTc4z-ZYwog"},"source":["model = MyGAN((256,256,3),D_PATH,G_PATH,True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y4nJRDESE8a6"},"source":["model.train(\"monet.npy\",\"/content/not/*.npy\")"],"execution_count":null,"outputs":[]}]}