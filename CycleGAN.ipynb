{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CycleGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRUDIk-Ktnkg"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D, Add\n",
        "from tensorflow.keras.layers import MaxPooling2D, UpSampling2D, Conv2D, Conv2DTranspose\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "import tensorflow as tf\n",
        "from random import randint\n",
        " \n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        " \n",
        "from tensorflow.python.framework.ops import disable_eager_execution, enable_eager_execution\n",
        "\n",
        "import random\n",
        "\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYhpfDF45iT5"
      },
      "source": [
        "import logging\n",
        "tf.get_logger().setLevel(logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgV-dU-BzPFm"
      },
      "source": [
        "MODELS_PATH = \"/content/gdrive/MyDrive/Projects/GDL Images/models/CycleGAN/\"\n",
        "D_A_PATH = MODELS_PATH+\"d_A.h5\"\n",
        "D_B_PATH = MODELS_PATH+\"d_B.h5\"\n",
        "G_AB_PATH = MODELS_PATH+\"g_AB.h5\"\n",
        "G_BA_PATH = MODELS_PATH+\"g_BA.h5\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBIbgao-eJw6"
      },
      "source": [
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "marVKyisVpwt"
      },
      "source": [
        "!ln -s \"/content/gdrive/MyDrive/Projects/GDL Images/dataset/monet.npy\" \"/content/monet.npy\"\n",
        "!ln -s \"/content/gdrive/MyDrive/Projects/GDL Images/dataset/not/\" \"/content/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDw6kqv22zuU"
      },
      "source": [
        "def loadNpy(path):\n",
        "  return np.load(path, mmap_mode='r')/255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWF8t2afsdKw"
      },
      "source": [
        "class DataRemember():\n",
        "  def __init__(self, maxsize=1000):\n",
        "    self.maxsize = maxsize\n",
        "    self.array = []\n",
        "    self.length = 0\n",
        "  \n",
        "  def AddToArray(self, add):\n",
        "    free_size = self.maxsize - len(self.array)\n",
        "    to_remove = len(add) - free_size\n",
        "    if len(self.array)>self.maxsize:\n",
        "      print(\"fuck\")\n",
        "    if to_remove > 0:\n",
        "      self.array = self.array[to_remove:]\n",
        "    for i in add:\n",
        "      self.array.append(i)\n",
        "    random.shuffle(self.array)\n",
        "    self.length = len(self.array)\n",
        "  \n",
        "  def ToNumpy(self):\n",
        "    return np.array(self.array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNlcj4-T9hYT"
      },
      "source": [
        "def buildGeneratorVGG19(input_shape, weight_init,name=\"generator\"):\n",
        "  vgg19 = VGG19(weights='imagenet',include_top=False)\n",
        "  def downSample(input_layer,filters,amount,max_pool=True):\n",
        "    y = input_layer\n",
        "    for _ in range(amount):\n",
        "      y = Conv2D(filters,kernel_size=3, strides=1, padding='same', activation=\"relu\")(y)\n",
        "    if max_pool:\n",
        "      y = MaxPooling2D(pool_size=2)(y)\n",
        "    return y\n",
        "  def upSample(input_layer,filters,amount,weight_init):\n",
        "    y = UpSampling2D(size=(2,2))(input_layer)\n",
        "    for _ in range(amount):\n",
        "      y = Conv2D(filters,kernel_size=3, strides=1, padding='same', activation=\"relu\",kernel_initializer=weight_init)(y)\n",
        "      y = BatchNormalization()(y)\n",
        "      y = Dropout(0.08)(y)\n",
        "    return y\n",
        "  input_layer = Input(shape=input_shape)\n",
        "  block1_out = downSample(input_layer,64,2)\n",
        "  block2_out = downSample(block1_out,128,2)\n",
        "  block3_out = downSample(block2_out,256,4)\n",
        "  block4_out = downSample(block3_out,512,4)\n",
        "  block5_out = downSample(block4_out,512,4,False)\n",
        "  y = Concatenate()([block5_out,block4_out])\n",
        "  y = upSample(y,512,4,weight_init)\n",
        "  y = Concatenate()([block3_out,y])\n",
        "  y = upSample(y,256,4,weight_init)\n",
        "  y = Concatenate()([block2_out,y])\n",
        "  y = upSample(y,128,2,weight_init)\n",
        "  y = Concatenate()([block1_out,y])\n",
        "  y = upSample(y,64,2,weight_init)\n",
        "  output = Conv2D(3,kernel_size=3, strides=1, padding='same', activation=\"sigmoid\",kernel_initializer=weight_init)(y)\n",
        "\n",
        "  model = Model(input_layer,output,name=name)\n",
        "\n",
        "  vgg19_layers = vgg19.layers\n",
        "  model_layers = model.layers\n",
        "  i = 0\n",
        "  while i<len(vgg19_layers)-1:\n",
        "    model_layers[i].set_weights(vgg19_layers[i].get_weights())\n",
        "    model_layers[i].trainable = False\n",
        "    i+=1\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_bQvyJ_wFni"
      },
      "source": [
        "def buildDiscriminator(input_shape,weight_init,name=\"disc\"):\n",
        "  def conv4(layer_input,filters,weight_init,norm=True,stride=2):\n",
        "    y = Conv2D(filters, kernel_size=4, strides=stride, padding='same', kernel_initializer = weight_init, activation=\"relu\")(layer_input)\n",
        "    if norm:\n",
        "      y = BatchNormalization()(y)\n",
        "    return y\n",
        "\n",
        "  img = Input(shape=input_shape)\n",
        "\n",
        "  y = conv4(img, 8, weight_init, norm = False)\n",
        "  y = conv4(y, 16, weight_init)\n",
        "  y = MaxPooling2D()(y)\n",
        "  y = conv4(y, 32, weight_init)\n",
        "  y = MaxPooling2D()(y)\n",
        "  #y = conv4(y, 32, weight_init)\n",
        "  output = Conv2D(1, kernel_size=2, strides=2, padding='same',kernel_initializer = weight_init, activation=\"sigmoid\")(y)\n",
        "\n",
        "  model = Model(img, output,name=name)\n",
        "  model.compile(\n",
        "    loss=\"mse\",\n",
        "    optimizer=Adam(),\n",
        "    metrics=['accuracy']\n",
        "  )\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amgOdmR2uhKo"
      },
      "source": [
        "class CycleGAN():\n",
        "  def __init__(self, \n",
        "      input_dim,\n",
        "      lambda_validation,\n",
        "      lambda_reconstr,\n",
        "      lambda_id,\n",
        "      d_A_path,\n",
        "      d_B_path,\n",
        "      g_AB_path,\n",
        "      g_BA_path,\n",
        "      load=False\n",
        "    ):\n",
        "    self.learning_rate = 0.02\n",
        "    self.input_dim = input_dim\n",
        "    self.lambda_validation = lambda_validation\n",
        "    self.lambda_reconstr = lambda_reconstr\n",
        "    self.lambda_id = lambda_id\n",
        "    self.d_A_path = d_A_path\n",
        "    self.d_B_path = d_B_path\n",
        "    self.g_AB_path = g_AB_path\n",
        "    self.g_BA_path = g_BA_path\n",
        "    \n",
        "    weight_init = RandomNormal(mean=0., stddev=0.02)\n",
        "    #build models\n",
        "    def LoadModelGenerator(name,path):\n",
        "      if load:\n",
        "        return tf.keras.models.load_model(path)\n",
        "      return buildGeneratorVGG19(input_dim,weight_init,name)\n",
        "    def LoadModelDiscriminator(name,path):\n",
        "      if load:\n",
        "        return tf.keras.models.load_model(path)\n",
        "      return buildDiscriminator(input_dim,weight_init,name)\n",
        "    self.d_A = LoadModelDiscriminator(\"d_A\",d_A_path)\n",
        "    self.d_B = LoadModelDiscriminator(\"d_B\",d_B_path)\n",
        "    self.g_AB = LoadModelGenerator(\"g_AB\",g_AB_path)\n",
        "    self.g_BA = LoadModelGenerator(\"g_BA\",g_BA_path)\n",
        "    self.disc_patch = self.d_A.output.shape[1:]\n",
        "\n",
        "    self.queue_not_B = DataRemember()\n",
        "\n",
        "    self.compile_models()\n",
        "      \n",
        "  def compile_models(self):\n",
        "    # Input images from both domains\n",
        "    img_A = Input(shape=self.input_dim)\n",
        "    img_B = Input(shape=self.input_dim)\n",
        "\n",
        "    # Translate images to the other domain\n",
        "    fake_B = self.g_AB(img_A)\n",
        "    fake_A = self.g_BA(img_B)\n",
        "\n",
        "    # Translate images back to original domain\n",
        "    reconstr_A = self.g_BA(fake_B)\n",
        "    reconstr_B = self.g_AB(fake_A)\n",
        "\n",
        "    # Identity mapping of images\n",
        "    img_A_id = self.g_BA(img_A)\n",
        "    img_B_id = self.g_AB(img_B)\n",
        "\n",
        "    # Discriminators determines validity of translated images\n",
        "    valid_A = self.d_A(fake_A)\n",
        "    valid_B = self.d_B(fake_B)\n",
        "    \n",
        "    # Combined model trains generators to fool discriminators\n",
        "    self.combined = Model(inputs=[img_A, img_B],\n",
        "                          outputs=[valid_A, valid_B,\n",
        "                                  reconstr_A, reconstr_B,\n",
        "                                  img_A_id, img_B_id])\n",
        "    self.combined.compile(loss=['mse', 'mse',\n",
        "                                'mae', 'mae',\n",
        "                                'mae', 'mae'],\n",
        "                          loss_weights=[self.lambda_validation, self.lambda_validation,\n",
        "                                        self.lambda_reconstr, self.lambda_reconstr,\n",
        "                                        self.lambda_id, self.lambda_id],\n",
        "                          optimizer=Adam(self.learning_rate, 0.5))\n",
        "\n",
        "  def TrainDiscriminators(self):\n",
        "    self.d_A.trainable = True\n",
        "    self.d_B.trainable = True\n",
        "    fake_A = self.g_BA.predict(self.original_B_data)\n",
        "    fake_B = self.g_AB.predict(self.original_A_data)\n",
        "\n",
        "    queue_not_A = DataRemember()\n",
        "    queue_not_A.AddToArray(self.original_B_data)\n",
        "    queue_not_A.AddToArray(fake_A)\n",
        "    self.queue_not_B.AddToArray(self.original_A_data)\n",
        "    self.queue_not_B.AddToArray(fake_B)\n",
        "\n",
        "    x = np.concatenate((self.original_A_data,queue_not_A.ToNumpy()))\n",
        "    zeros = np.zeros((queue_not_A.length,)+self.disc_patch)\n",
        "    y = np.concatenate((self.d_A_disc_ones,zeros))\n",
        "    while True:\n",
        "      self.d_A.fit(x,y,shuffle=True,epochs=10,batch_size=32)\n",
        "      self.d_A.save(self.d_A_path)\n",
        "      inp = input(\"Train Again? y for yes \")\n",
        "      if len(inp)==0:\n",
        "        break\n",
        "\n",
        "    x = np.concatenate((self.original_B_data,self.queue_not_B.ToNumpy()))\n",
        "    zeros = np.zeros((self.queue_not_B.length,)+self.disc_patch)\n",
        "    y = np.concatenate((self.d_B_disc_ones,zeros))\n",
        "    while True:\n",
        "      self.d_B.fit(x,y,shuffle=True,epochs=10,batch_size=32)\n",
        "      self.d_B.save(self.d_B_path)\n",
        "      inp = input(\"Train Again? y for yes \")\n",
        "      if len(inp)==0:\n",
        "        break\n",
        "\n",
        "  def TrainGenerators(self):\n",
        "    # For the combined model we will only train the generators\n",
        "    self.d_A.trainable = False\n",
        "    self.d_B.trainable = False\n",
        "    self.combined.fit([self.original_A_data,self.original_B_data],\n",
        "                      [self.d_A_disc_ones,self.d_B_disc_ones,\n",
        "                       self.original_A_data,self.original_B_data,\n",
        "                       self.original_A_data,self.original_B_data],\n",
        "                      shuffle=True,epochs=2,batch_size=2)\n",
        "\n",
        "  def SaveModels(self):\n",
        "    self.d_A.trainable = True\n",
        "    self.d_B.trainable = True\n",
        "    self.d_A.save(self.d_A_path)\n",
        "    self.d_B.save(self.d_B_path)\n",
        "    self.g_AB.save(self.g_AB_path)\n",
        "    self.g_BA.save(self.g_BA_path)\n",
        "\n",
        "  def train(self, path_A, path_B, times=5):\n",
        "    files = glob.glob(path_B)\n",
        "    random.shuffle(files)\n",
        "    self.original_A_data = loadNpy(path_A)\n",
        "    A_len = self.original_A_data.shape[0]\n",
        "    self.d_A_disc_ones = np.ones((self.original_A_data.shape[0],)+self.disc_patch)\n",
        "    for B_file in files:\n",
        "      B_data = loadNpy(B_file)\n",
        "      i = B_data.shape[0]//A_len\n",
        "      for j in range(i):\n",
        "        self.original_B_data = B_data[j*A_len:(j+1)*A_len]\n",
        "        self.d_B_disc_ones = np.ones((self.original_B_data.shape[0],)+self.disc_patch)\n",
        "        self.TrainDiscriminators()\n",
        "        self.TrainGenerators()\n",
        "        self.ShowProgress()\n",
        "        self.ShowProgress(False)\n",
        "        self.SaveModels()\n",
        "\n",
        "  def ShowProgress(self, model_AB=True):\n",
        "    if model_AB:\n",
        "      model = self.g_AB\n",
        "      i = random.randint(0, self.original_A_data.shape[0])\n",
        "      img = self.original_A_data[i]\n",
        "      print(\"A->B:\")\n",
        "    else:\n",
        "      model = self.g_BA\n",
        "      i = random.randint(0, self.original_B_data.shape[0])\n",
        "      img = self.original_B_data[i]\n",
        "      print(\"B->A:\")\n",
        "    generated = model.predict(np.array([img]))[0]\n",
        "    generated = generated*256\n",
        "    print(\"original:\")\n",
        "    cv2_imshow(img*256)\n",
        "    print(\"generated:\")\n",
        "    cv2_imshow(generated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLdw_7sbyLKD"
      },
      "source": [
        "gan = CycleGAN(\n",
        "  (256,256,3),\n",
        "  1,2,2,\n",
        "  D_A_PATH,\n",
        "  D_B_PATH,\n",
        "  G_AB_PATH,\n",
        "  G_BA_PATH,\n",
        "  True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSZQCqby0ajX"
      },
      "source": [
        "gan.train(\"monet.npy\",\"/content/not/*.npy\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}